{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMo616B3hh8CcCWB68bWQC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TayJen/hackathon_algu2022/blob/master/H_ALGU_2022_Advance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFVPXkf-r0R-",
        "outputId": "27f5287c-2548-4a95-d837-33c6b8650026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/data_drive/H_Vladivostok2022\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajt0xnRYr_aN",
        "outputId": "0ddb9a0d-872f-4219-ccac-0648d9d21a39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/data_drive/H_Vladivostok2022\n",
            "/content/drive/Shareddrives/data_drive/H_Vladivostok2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт библиотек"
      ],
      "metadata": {
        "id": "AAfLrhOhsaHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Для работы с данными\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Для визуализации\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Для работы с текстом\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "# Для моделей\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Метрика\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.facecolor'] = 'white'\n",
        "np.random.seed(59)\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7rpBM58r_cd",
        "outputId": "14453c43-5cfc-42aa-ed4b-bdc7377c6f9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Работа с данными"
      ],
      "metadata": {
        "id": "eBTTuH9QtLce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем данные"
      ],
      "metadata": {
        "id": "KlN-dv7tu7hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./data/train/train_issues.csv\")"
      ],
      "metadata": {
        "id": "P4sCxLgxu8-H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работа со временем"
      ],
      "metadata": {
        "id": "TgBRw49zt6ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['created_time'] = pd.to_datetime(df['created'], format='%Y-%m-%d %H:%M:%S')\n",
        "df['year'] = df['created_time'].dt.year\n",
        "df['month'] = df['created_time'].dt.month\n",
        "df['day'] = df['created_time'].dt.day\n",
        "df['hour'] = df['created_time'].dt.hour\n",
        "df['minute'] = df['created_time'].dt.minute\n",
        "\n",
        "df.drop(['created', 'created_time'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "vbcT8f2Hr_en"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работа с ключем задачи"
      ],
      "metadata": {
        "id": "kpxIpPyDt8Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['key_name'] = df['key'].apply(lambda x: x.split('-')[0])\n",
        "df['key_num'] = df['key'].apply(lambda x: x.split('-')[1])\n",
        "\n",
        "df.drop(['key', 'key_num'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "QdO-WxwWr_go"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работа с описанием задачи"
      ],
      "metadata": {
        "id": "E21LgklOt9v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = Mystem()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "w_tokenizer = WhitespaceTokenizer()"
      ],
      "metadata": {
        "id": "IijOGXhuuMNM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того чтобы лемматизация работала, необходимо раскомментировать первые две строки при первом запуске"
      ],
      "metadata": {
        "id": "c8an7QotuRJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "# !tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "metadata": {
        "id": "qx5o7-dMuNG2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_text(text):\n",
        "    t = re.sub(r'[^a-zA-Z0-9а-яА-ЯёЁ\\' ]', ' ', text)\n",
        "    t = ' '.join(t.split())\n",
        "    return t\n",
        "\n",
        "def lemmatize_text_rus(text):\n",
        "    tokens = m.lemmatize(text.lower())\n",
        "    tokens = [token for token in tokens if token != '\\n']\n",
        "    text = \" \".join(tokens)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:         \n",
        "        return None\n",
        "\n",
        "def lemmatize_with_pos_eng(text):\n",
        "    pos_tagged = nltk.pos_tag(w_tokenizer.tokenize(text))\n",
        "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:       \n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, pos=tag))\n",
        "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
        "    return lemmatized_sentence\n",
        "\n",
        "\n",
        "df['clear_summary'] = df['summary'].apply(clear_text)\n",
        "df['clear_summary'] = df['clear_summary'].str.lower()\n",
        "df['lemm_summary'] = df['clear_summary'].apply(lemmatize_text_rus)\n",
        "df['lemm_summary'] = df['lemm_summary'].apply(lemmatize_with_pos_eng)\n",
        "\n",
        "df.drop(['summary', 'clear_summary'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Y7OEgskar_ix"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Преобразуем таргет, чтобы исключить выбросы и минимизировать разброс"
      ],
      "metadata": {
        "id": "XEaCl0e2vQpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['log_target'] = np.log(df['overall_worklogs'])"
      ],
      "metadata": {
        "id": "lsZqkcbCr_kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ga2DYeBvr_m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uIDlv_26r_qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmJk7nyNr_r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWGVcf9tr_tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWpusyvcr_vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJlQMXytr_x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RQHUI1or_zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9eH_bbvr_1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}